{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ca156e0-36d5-4b97-916f-ade263e099e3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Overview\n",
    "That series of notebooks showcases a few lessons learnt while using Spark\n",
    "(and [Delta Lake](https://delta.io)) to process data. DataBricks is used here for convenience.\n",
    "However, any other notebook and/or compute engine may be used the same way,\n",
    "as the concepts presented here are based only on the open source version\n",
    "of both Spark and Delta Lake.\n",
    "\n",
    "## References\n",
    "* [GitHub - DataBricks examples](https://github.com/data-engineering-helpers/databricks-examples) (Git repository hosting this notebook)\n",
    "* [Delta Lake homepage](https://delta.io)\n",
    "* [Geonames - Data dump folder](http://download.geonames.org/export/dump/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e612f7ad-d6af-4642-9104-329abfae956b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "# Geonames data\n",
    "geo_hierarchy_remote_basepath: str = \"https://github.com/data-engineering-helpers/databricks-examples/raw/main/data/geonames\"\n",
    "geo_hierarchy_local_basepath_wo_dbfs: str = \"/tmp/geonames\"\n",
    "geo_hierarchy_local_basepath: str = f\"/dbfs{geo_hierarchy_local_basepath_wo_dbfs}\"\n",
    "geo_hierarchy_prefix: str = \"geonames-hierarchy\"\n",
    "\n",
    "# Local temporary folder\n",
    "local_dir: pathlib.Path = pathlib.Path(geo_hierarchy_local_basepath)\n",
    "\n",
    "# Max level of hierarchy\n",
    "max_level: int = 12\n",
    "\n",
    "# Organization table\n",
    "geo_hierarchy_table_ref: str = \"database.geo_hierarchy\" # <--- Replace here with your own Hive metastore table ID\n",
    "\n",
    "# List of extract dates from CSV extract\n",
    "g_date_list: list[str] = [\"20230602\",]\n",
    "g_latest_date: str = \"2023-06-02\"\n",
    "\n",
    "# List of extract dates from geo hierarchy table\n",
    "g_date_list_from_table: list[str] = None\n",
    "g_latest_date_from_table: str = None\n",
    "\n",
    "# Diff list: extract dates existing as CSV data files but not yet in the geo hierarchy table\n",
    "g_date_list_diff: list[str] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e9d1665-c2db-4d3b-a029-73d5508a832e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "geo_hierarchy_ddl_drop = \"\"\"\n",
    "drop table if exists database.geo_hierarchy;\n",
    "\"\"\"\n",
    "\n",
    "geo_hierarchy_ddl_create = \"\"\"\n",
    "create or replace table database.geo_hierarchy (\n",
    " extract_date date,\n",
    " parent integer,\n",
    " child integer,\n",
    " type string\n",
    ")\n",
    "using delta\n",
    "tblproperties (delta.enableChangeDataFeed=true)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40c5ad54-beaa-4c1b-8ea8-174619dae4ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import shutil\n",
    "\n",
    "def downloadCSVFiles(debug: bool = False) -> None:\n",
    "    # Remove any previous downloaded data\n",
    "    shutil.rmtree(path=local_dir)\n",
    "    local_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Download the CSV data files to the local /tmp folder\n",
    "    for extract_date in g_date_list:\n",
    "        remote_filename: str = f\"{geo_hierarchy_prefix}-{extract_date}.csv.bz2\"\n",
    "        remote_file: str = f\"{geo_hierarchy_remote_basepath}/{remote_filename}\"\n",
    "        local_file: str = f\"{geo_hierarchy_local_basepath}/{remote_filename}\"\n",
    "        urllib.request.urlretrieve(remote_file, local_file)\n",
    "        \n",
    "    # Check the downloaded files\n",
    "    if debug:\n",
    "        local_file_list = [fp for fp in local_dir.iterdir()]    \n",
    "        print(local_file_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "983cd568-b42b-468a-afe5-7dc0ac0c0bf5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "downloadCSVFiles(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73063bd7-7652-445c-9385-69201b6cc781",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.dataframe\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Raw schema (without extract date)\n",
    "geoHierarchySchema = (\n",
    "    StructType()\n",
    "    .add(StructField(\"parent\", IntegerType(), True))\n",
    "    .add(StructField(\"child\", IntegerType(), True))\n",
    "    .add(StructField(\"type\", StringType(), True))\n",
    ")\n",
    "\n",
    "def get_list_from_df(\n",
    "                    key: str,\n",
    "                    input_df: pyspark.sql.dataframe.DataFrame,\n",
    "                    debug: bool=False\n",
    "                    ) -> list[str]:\n",
    "    row_list = input_df.select(key).collect()\n",
    "    raw_list: list[str] = [row.uid for row in row_list]\n",
    "    return raw_list\n",
    "\n",
    "def elt(\n",
    "        extract_date: str,\n",
    "        local_path: str,\n",
    "        debug: bool=False\n",
    "        ) -> pyspark.sql.dataframe.DataFrame:\n",
    "    geo_adm = spark.read.option(\"sep\", \"\\t\").option(\"header\", \"false\").schema(geoHierarchySchema).csv(local_path)\n",
    "\n",
    "    # Add the extraction date\n",
    "    geo_adm = geo_adm.withColumn(\"extract_date\", lit(extract_date)).withColumn(\"extract_date\", to_date(\"extract_date\", \"yyyy-MM-dd\"))\n",
    "\n",
    "    return geo_adm\n",
    "\n",
    "def elt_all(\n",
    "            date_list: list[str],\n",
    "            debug: bool=False\n",
    "            ) -> dict[str, pyspark.sql.dataframe.DataFrame]:\n",
    "    # Initialize the global list of geo hierarchy DataFrames\n",
    "    geo_adm_list: dict[str, pyspark.sql.dataframe.DataFrame] = dict()\n",
    "\n",
    "    # Browse every data file and load a DataFrame with it\n",
    "    for extract_date in date_list:        \n",
    "        extract_date_compact: str = extract_date.replace(\"-\", \"\")\n",
    "        local_path: str = f\"{geo_hierarchy_local_basepath_wo_dbfs}/{geo_hierarchy_prefix}-{extract_date_compact}.csv.bz2\"\n",
    "        geo_adm = elt(extract_date=extract_date, local_path=local_path, debug=debug)\n",
    "        if debug: print(f\"Extract date: {extract_date} - Nb of records: {geo_adm.count()}\")\n",
    "        geo_adm_list[extract_date] = geo_adm\n",
    "\n",
    "    return geo_adm_list\n",
    "\n",
    "def save_as_table(\n",
    "                  input_geo_list: dict[str, pyspark.sql.dataframe.DataFrame],\n",
    "                  date_list: list[str],\n",
    "                  debug: bool=False\n",
    "                  ) -> None:\n",
    "    # Debug\n",
    "    if debug: print(f\"Saving the DataFrames, parsed from CSV data files, for the following ({len(date_list)}) extract dates: {date_list}\")\n",
    "\n",
    "    # Save every extract as a version in the geo hierarchy Delta table,\n",
    "    # and store the mapping in the dedicated table\n",
    "    for extract_date in sorted(date_list):\n",
    "        # As, in the widget, the empty string is required (in case there is no extract date in the diff),\n",
    "        # it must be filtered out here\n",
    "        if extract_date == \"\":\n",
    "            continue\n",
    "\n",
    "        geo_adm = input_geo_list.get(extract_date, None)\n",
    "        if not geo_adm:\n",
    "            print(f\"Error - For some reason, there is no DataFrame, parsed from CSV data file, for the {extract_date} extract date. List of extract dates for the DataFrames: {input_geo_list.keys()}\")\n",
    "            continue\n",
    "\n",
    "        if debug: print(f\"Saving {extract_date} DataFrame as {geo_hierarchy_table_ref} table...\")\n",
    "        geo_adm.write.format(\"delta\").mode(\"append\").saveAsTable(geo_hierarchy_table_ref)\n",
    "        if debug: print(f\"Saved {extract_date} DataFrame as {geo_hierarchy_table_ref} table...\")\n",
    "    #\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3d9690e-8352-4184-8be7-17d9047515fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re, datetime\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "def derive_diff(\n",
    "                date_list_from_table: list[str],\n",
    "                date_list: list[str],\n",
    "                debug=False\n",
    "                ) -> (str, list[str]):\n",
    "    diff_list = list(set(date_list_from_table).symmetric_difference(set(date_list)))\n",
    "    diff_list = sorted(diff_list, reverse=True)\n",
    "    # Extract the latest date (first element of the reverse-sorted list)\n",
    "    latest_date = diff_list[0] if diff_list else None\n",
    "\n",
    "    # Debug\n",
    "    if debug:\n",
    "        print(f\"Differences in both lists ({len(diff_list)} records): {diff_list}\")\n",
    "        if latest_date:\n",
    "            print(f\"  => latest extract date from diff list: {latest_date}\")\n",
    "\n",
    "    return latest_date, diff_list\n",
    "\n",
    "def list_from_main_table(\n",
    "              debug=False\n",
    "              ) -> (str, list[str]):\n",
    "    # Build a DataFrame from the main table\n",
    "    geo_all = spark.read.table(geo_hierarchy_table_ref)\n",
    "\n",
    "    # List all the extract dates and build a pure Python list\n",
    "    extract_date_df_list_from_table = geo_all.select(col(\"extract_date\")).distinct().collect()\n",
    "    date_list = [extract_date_row.extract_date.strftime(\"%Y-%m-%d\") for extract_date_row in extract_date_df_list_from_table]\n",
    "\n",
    "    # Sort the dates, just in case the DataFrame would not do it already\n",
    "    date_list = sorted(date_list, reverse=True)\n",
    "    # Extract the latest date (first element of the reverse-sorted list)\n",
    "    latest_date = date_list[0] if date_list else None\n",
    "\n",
    "    # Debug\n",
    "    if debug:\n",
    "        print(f\"List of extract dates from the geo hierarchy table ({len(date_list)} records): {date_list}\")\n",
    "        if latest_date:\n",
    "            print(f\"  => latest extract date from geo hierarchy table: {latest_date}\")\n",
    "\n",
    "    return latest_date, date_list\n",
    "\n",
    "def list_from_csv_extracts(\n",
    "                           debug=False\n",
    "                           ) -> (str, list[str]):\n",
    "    \"\"\"Derive the dates for which there are CSV extract files\"\"\"\n",
    "    date_list: list[str] = []\n",
    "\n",
    "    for csv_extract in local_dir.glob(f\"{geo_hierarchy_prefix}-*.csv.bz2\"):\n",
    "        csv_filename: str = csv_extract.name\n",
    "        m = re.match(r\"geonames-hierarchy-(\\d+).csv.bz2\", csv_filename)\n",
    "        extract_date_str: str = m.group(1)\n",
    "        extract_date: datetime.date = datetime.datetime.strptime(extract_date_str, \"%Y%m%d\").date().strftime(\"%Y-%m-%d\")\n",
    "        date_list.append(extract_date)\n",
    "\n",
    "    # Sort the dates, just in case Cloudpathlib or AWS S3 would not do it already\n",
    "    date_list = sorted(date_list, reverse=True)\n",
    "    # Extract the latest date (first element of the reverse-sorted list)\n",
    "    latest_date = date_list[0] if date_list else None\n",
    "\n",
    "    # Debug\n",
    "    if debug:\n",
    "        print(f\"List of extract dates from the '{local_dir}/' directory ({len(date_list)} records): {date_list}\")\n",
    "\n",
    "        if latest_date:\n",
    "            print(f\"  => latest extract date from CSV data files: {latest_date}\")\n",
    "\n",
    "    return latest_date, date_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87a4ad5c-76cd-4685-9227-ed3006c9a92a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extracts from the main table\n",
    "g_latest_date_from_table, g_date_list_from_table = list_from_main_table(debug=True)\n",
    "\n",
    "# Extracts from the CSV data files on the S3 folder\n",
    "g_latest_date, g_date_list = list_from_csv_extracts(debug=True)\n",
    "\n",
    "# Difference between both\n",
    "g_latest_date_diff, g_date_list_diff = derive_diff(date_list_from_table=g_date_list_from_table, date_list=g_date_list, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3be60dad-830f-4128-87bf-11212e94ab32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.removeAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00d00dc9-de1f-45c3-85df-0ea51c21cdf8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "dbutils.widgets.text(\"latest_extract\", g_latest_date)\n",
    "\n",
    "#\n",
    "g_date_list_from_table_for_widget = g_date_list_from_table.copy()\n",
    "if len(g_date_list_from_table_for_widget)==0:\n",
    "    g_latest_date_from_table = \"\"\n",
    "    g_date_list_from_table_for_widget.append(g_latest_date_from_table)\n",
    "dbutils.widgets.dropdown(\"extracts\", g_latest_date_from_table, g_date_list_from_table_for_widget)\n",
    "\n",
    "#\n",
    "dbutils.widgets.multiselect(\"csv_extracts\", g_latest_date, g_date_list)\n",
    "g_date_list_diff_for_widget = g_date_list_diff.copy()\n",
    "if len(g_date_list_diff_for_widget)==0:\n",
    "    g_latest_date_diff = \"\"\n",
    "    g_date_list_diff_for_widget.append(g_latest_date_diff)\n",
    "else:\n",
    "    g_latest_date_diff = g_date_list_diff_for_widget[0]\n",
    "dbutils.widgets.multiselect(\"new_extracts\", g_latest_date_diff, g_date_list_diff_for_widget)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a55568c-d416-4ec6-acb6-082ccb35240b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "geo_adm_list = elt_all(date_list=g_date_list, debug=False)\n",
    "print(f\"Number of extracts: {len(geo_adm_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1184124b-57f5-4e87-b800-c11e0b99f772",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "latest_extract_date = getArgument(\"latest_extract\")\n",
    "print(f\"Pick up the extract for {latest_extract_date}\")\n",
    "geo_adm_last = geo_adm_list.get(latest_extract_date)\n",
    "display(geo_adm_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45da5473-d08b-4a02-b8cf-192d22694a5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Delete the table\n",
    "spark.sql(geo_hierarchy_ddl_drop)\n",
    "\n",
    "# (Re-)create the table\n",
    "spark.sql(geo_hierarchy_ddl_create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5452dc7-489b-494e-b22d-bb331f3941f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "save_as_table(input_geo_list=geo_adm_list, date_list=g_date_list, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4d470cc-1c31-4ac4-9e50-fbf209d1a21d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "geo_all = spark.read.table(geo_hierarchy_table_ref).filter(col(\"extract_date\") == lit(latest_extract_date))\n",
    "display(geo_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c5cf217-43ac-4d65-ac15-03eb856d9150",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Learnt lessons with Spark and Delta",
   "widgets": {
    "csv_extracts": {
     "currentValue": "2023-06-02",
     "nuid": "28f14c56-6907-4b83-a695-bb2890738858",
     "widgetInfo": {
      "widgetType": "multiselect",
      "defaultValue": "2023-06-02",
      "label": null,
      "name": "csv_extracts",
      "options": {
       "widgetType": "dropdown",
       "choices": [
        "2023-06-02"
       ]
      }
     }
    },
    "extracts": {
     "currentValue": "2023-06-02",
     "nuid": "7ed83882-2f78-4fc8-a2e9-4c796f5c1f75",
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "2023-06-02",
      "label": null,
      "name": "extracts",
      "options": {
       "widgetType": "dropdown",
       "choices": [
        "2023-06-02"
       ]
      }
     }
    },
    "latest_extract": {
     "currentValue": "2023-06-02",
     "nuid": "d81bf7fe-c27c-47a7-97bb-260227924df1",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "2023-06-02",
      "label": null,
      "name": "latest_extract",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    },
    "new_extracts": {
     "currentValue": "",
     "nuid": "1b4274ec-6cfb-4566-92db-a5c32565aa41",
     "widgetInfo": {
      "widgetType": "multiselect",
      "defaultValue": "",
      "label": null,
      "name": "new_extracts",
      "options": {
       "widgetType": "dropdown",
       "choices": [
        ""
       ]
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
