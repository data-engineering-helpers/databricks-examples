{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cf06739-6c1b-47ce-a277-9a6214546b17",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Demos - Geo - Plotly and streamlit\n",
    "=================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f9bacae-a5d2-4b8b-9b6e-a334567fa151",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "* [Medium - How to Create a Simple GIS Map with Plotly and Streamlit](https://towardsdatascience.com/how-to-create-a-simple-gis-map-with-plotly-and-streamlit-7732d67b84e2)\n",
    "  + Date: Dec. 2023\n",
    "  + Author: Alan Jones\n",
    "  ([Alan Jones on Medium](https://medium.com/@alan-jones),\n",
    "  [Alan Jones on LinkedIn](https://www.linkedin.com/in/alan-jones-032699100/))\n",
    "  + Publisher: Medium TowardsDataScience\n",
    "  + [GitHub - repository with the code for that notebook](https://github.com/alanjones2/st-choropleth/tree/main)\n",
    "* [GitHub - Data Engineering helpers - Material about geo-analysis with Python](https://github.com/data-engineering-helpers/ks-cheat-sheets/blob/main/programming/python/geo/README.md)\n",
    "* [Demos - Geo - Plotly and streamlit (this notebook)](https://github.com/data-engineering-helpers/databricks-examples/blob/main/ipython-notebooks/demos-geo-plotly-and-streamlit.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad4a246e-7996-40e7-a231-429f815cc4be",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting streamlit\n  Downloading streamlit-1.29.0-py2.py3-none-any.whl (8.4 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.4/8.4 MB 47.2 MB/s eta 0:00:00\nRequirement already satisfied: plotly in /databricks/python3/lib/python3.10/site-packages (5.9.0)\nRequirement already satisfied: pillow<11,>=7.1.0 in /databricks/python3/lib/python3.10/site-packages (from streamlit) (9.2.0)\nRequirement already satisfied: importlib-metadata<7,>=1.4 in /databricks/python3/lib/python3.10/site-packages (from streamlit) (4.11.3)\nRequirement already satisfied: numpy<2,>=1.19.3 in /databricks/python3/lib/python3.10/site-packages (from streamlit) (1.21.5)\nRequirement already satisfied: pyarrow>=6.0 in /databricks/python3/lib/python3.10/site-packages (from streamlit) (8.0.0)\nRequirement already satisfied: cachetools<6,>=4.0 in /databricks/python3/lib/python3.10/site-packages (from streamlit) (4.2.4)\nRequirement already satisfied: tornado<7,>=6.0.3 in /databricks/python3/lib/python3.10/site-packages (from streamlit) (6.1)\nCollecting watchdog>=2.1.5\n  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 82.1/82.1 kB 14.1 MB/s eta 0:00:00\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.10/site-packages (from streamlit) (8.0.4)\nRequirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\nCollecting rich<14,>=10.14.0\n  Downloading rich-13.7.0-py3-none-any.whl (240 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 240.6/240.6 kB 42.2 MB/s eta 0:00:00\nRequirement already satisfied: packaging<24,>=16.8 in /databricks/python3/lib/python3.10/site-packages (from streamlit) (21.3)\nRequirement already satisfied: pandas<3,>=1.3.0 in /databricks/python3/lib/python3.10/site-packages (from streamlit) (1.4.4)\nRequirement already satisfied: tenacity<9,>=8.1.0 in /databricks/python3/lib/python3.10/site-packages (from streamlit) (8.1.0)\nRequirement already satisfied: protobuf<5,>=3.20 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages (from streamlit) (4.25.1)\nCollecting pydeck<1,>=0.8.0b4\n  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.8/4.8 MB 96.9 MB/s eta 0:00:00\nCollecting tzlocal<6,>=1.1\n  Downloading tzlocal-5.2-py3-none-any.whl (17 kB)\nRequirement already satisfied: python-dateutil<3,>=2.7.3 in /databricks/python3/lib/python3.10/site-packages (from streamlit) (2.8.2)\nRequirement already satisfied: requests<3,>=2.27 in /databricks/python3/lib/python3.10/site-packages (from streamlit) (2.28.1)\nRequirement already satisfied: typing-extensions<5,>=4.3.0 in /databricks/python3/lib/python3.10/site-packages (from streamlit) (4.3.0)\nRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /databricks/python3/lib/python3.10/site-packages (from streamlit) (3.1.27)\nCollecting validators<1,>=0.2\n  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\nCollecting toml<2,>=0.10.1\n  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\nCollecting altair<6,>=4.0\n  Downloading altair-5.2.0-py3-none-any.whl (996 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 996.9/996.9 kB 87.6 MB/s eta 0:00:00\nRequirement already satisfied: jsonschema>=3.0 in /databricks/python3/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (4.16.0)\nCollecting toolz\n  Downloading toolz-0.12.0-py3-none-any.whl (55 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 55.8/55.8 kB 12.7 MB/s eta 0:00:00\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (2.11.3)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.10/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.10)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.10/site-packages (from importlib-metadata<7,>=1.4->streamlit) (3.8.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.10/site-packages (from packaging<24,>=16.8->streamlit) (3.0.9)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.10/site-packages (from pandas<3,>=1.3.0->streamlit) (2022.1)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<3,>=2.7.3->streamlit) (1.16.0)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (2022.9.14)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (1.26.11)\nRequirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (2.0.4)\nCollecting markdown-it-py>=2.2.0\n  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.5/87.5 kB 19.8 MB/s eta 0:00:00\nCollecting pygments<3.0.0,>=2.13.0\n  Downloading pygments-2.17.2-py3-none-any.whl (1.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 80.9 MB/s eta 0:00:00\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.0)\nRequirement already satisfied: MarkupSafe>=0.23 in /databricks/python3/lib/python3.10/site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.0.1)\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /databricks/python3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.0)\nRequirement already satisfied: attrs>=17.4.0 in /databricks/python3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (21.4.0)\nCollecting mdurl~=0.1\n  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\nInstalling collected packages: watchdog, validators, tzlocal, toolz, toml, pygments, mdurl, pydeck, markdown-it-py, rich, altair, streamlit\n  Attempting uninstall: pygments\n    Found existing installation: Pygments 2.11.2\n    Not uninstalling pygments at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-d16b1a70-2dbc-4953-a70b-290eeb30b0d9\n    Can't uninstall 'Pygments'. No files were found to uninstall.\nSuccessfully installed altair-5.2.0 markdown-it-py-3.0.0 mdurl-0.1.2 pydeck-0.8.1b0 pygments-2.17.2 rich-13.7.0 streamlit-1.29.0 toml-0.10.2 toolz-0.12.0 tzlocal-5.2 validators-0.22.0 watchdog-3.0.0\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install streamlit plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10c2a2ba-ec05-44d9-89db-5ccdb9dd292b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c548316-0072-40f6-a74c-582fdd6d228f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd # pyspark.pandas as pd\n",
    "import streamlit as st\n",
    "import plotly.express as px\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "041b54ff-6d4c-48cd-9164-24d90528987d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Data\n",
    "Our final dashboard will use data about CO2 emissions. This data is derived from tables\n",
    "in a GitHub repository belonging to\n",
    "[Our World in Data](https://ourworldindata.org/).\n",
    "OWID is a great source of data and analysis which I have used many times\n",
    "(_e.g._ [New Data Demonstrates that 2023 was the Hottest Summer Ever](https://medium.com/towards-data-science/new-data-demonstrates-that-2023-was-the-hottest-summer-ever-d92d500a8f01)).\n",
    "\n",
    "The data that I have copied from OWID represents the CO2 emissions for countries since 1750.\n",
    "The original data contains far more information than we need, so I have created subsets\n",
    "of the data and stored them in the app. You will find them in the data folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b399452-14b2-4548-859c-53c759017a39",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n\r  3  806k    3 31836    0     0  69739      0  0:00:11 --:--:--  0:00:11 69739\r100  806k  100  806k    0     0  1657k      0 --:--:-- --:--:-- --:--:-- 25.2M\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n\r100   468  100   468    0     0    895      0 --:--:-- --:--:-- --:--:--   895\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",Entity,Code,Year,Annual CO₂ emissions\n0,Afghanistan,AFG,1949,14656.0\n1,Afghanistan,AFG,1950,84272.0\n2,Afghanistan,AFG,1951,91600.0\n3,Afghanistan,AFG,1952,91600.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n\r100  622k  100  622k    0     0  1567k      0 --:--:-- --:--:-- --:--:-- 1567k\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dbfs/tmp/demo-streamlit:\ntotal 8.0K\ndrwxrwxrwx 2 nobody nogroup 4.0K Dec 28 18:04 data/\ndrwxrwxrwx 2 nobody nogroup 4.0K Dec 28 18:04 geo/\n\n/dbfs/tmp/demo-streamlit/data:\ntotal 808K\n-rwxrwxrwx 1 nobody nogroup  468 Dec 28  2023 Australian-Bureau-of-Statistics.csv*\n-rwxrwxrwx 1 nobody nogroup 807K Dec 28 18:04 co2_total.csv*\n\n/dbfs/tmp/demo-streamlit/geo:\ntotal 623K\n-rwxrwxrwx 1 nobody nogroup 623K Dec 28  2023 australia.geojson*\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "\n",
    "rm -rf /dbfs/tmp/demo-streamlit\n",
    "mkdir -p /dbfs/tmp/demo-streamlit/{data,geo}\n",
    "curl -kL https://github.com/alanjones2/st-choropleth/raw/main/data/co2_total.csv -o /dbfs/tmp/demo-streamlit/data/co2_total.csv\n",
    "curl -kL https://github.com/alanjones2/st-choropleth/raw/main/data/Australian%20Bureau%20of%20Statistics.csv -o /dbfs/tmp/demo-streamlit/data/Australian-Bureau-of-Statistics.csv\n",
    "head -5 /dbfs/tmp/demo-streamlit/data/co2_total.csv\n",
    "curl -kL https://github.com/alanjones2/st-choropleth/raw/main/geo/australia.geojson -o /dbfs/tmp/demo-streamlit/geo/australia.geojson\n",
    "ls -lFhR /dbfs/tmp/demo-streamlit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1f94e45-9368-499c-a38a-5bbd72a442a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_total = pd.read_csv(\"/dbfs/tmp/demo-streamlit/data/co2_total.csv\")\n",
    "df_total_2021 = df_total[df_total['Year']==2021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e78a95a9-d391-4ca4-a469-52eb7ec63091",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "col = 'Annual CO₂ emissions'\n",
    "max = df_total_2021[col].max()\n",
    "min = df_total_2021[col].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88b349f1-5595-45c4-95ac-8d5f624c0ad8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig = px.choropleth(df_total_2021, \n",
    "                    locations=\"Code\",\n",
    "                    color=col,\n",
    "                    hover_name=\"Entity\",\n",
    "                    range_color=(min,max)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c918cc27-87ac-4a5d-9362-2dd5d49a6118",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig = px.choropleth(df_total_2021, \n",
    "                    locations=\"Code\",\n",
    "                    scope = 'europe',\n",
    "                    color=col,\n",
    "                    hover_name=\"Entity\",\n",
    "                    range_color=(min,max),\n",
    "                    title = 'Europe'\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06811666-36cd-44f2-9199-9075e9371a97",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4483247640063516>, line 7\u001B[0m\n",
       "\u001B[1;32m      4\u001B[0m oz \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(f)\n",
       "\u001B[1;32m      6\u001B[0m df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/dbfs/tmp/demo-streamlit/data/Australian-Bureau-of-Statistics.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[0;32m----> 7\u001B[0m fig \u001B[38;5;241m=\u001B[39m px\u001B[38;5;241m.\u001B[39mchoropleth(df, geojson\u001B[38;5;241m=\u001B[39moz, \n",
       "\u001B[1;32m      8\u001B[0m                     color\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPopulation at 31 March 2023 (\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m000)\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m      9\u001B[0m                     locations\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mState\u001B[39m\u001B[38;5;124m\"\u001B[39m, \n",
       "\u001B[1;32m     10\u001B[0m                     featureidkey\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproperties.name\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m     11\u001B[0m                     color_continuous_scale\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReds\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m     12\u001B[0m                     range_color\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m10000\u001B[39m),\n",
       "\u001B[1;32m     13\u001B[0m                     fitbounds \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgeojson\u001B[39m\u001B[38;5;124m'\u001B[39m,\n",
       "\u001B[1;32m     14\u001B[0m                     template \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mplotly_white\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
       "\u001B[1;32m     15\u001B[0m                    )\n",
       "\u001B[1;32m     16\u001B[0m st\u001B[38;5;241m.\u001B[39mplotly_chart(fig)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/plotly/express/_chart_types.py:1091\u001B[0m, in \u001B[0;36mchoropleth\u001B[0;34m(data_frame, lat, lon, locations, locationmode, geojson, featureidkey, color, facet_row, facet_col, facet_col_wrap, facet_row_spacing, facet_col_spacing, hover_name, hover_data, custom_data, animation_frame, animation_group, category_orders, labels, color_discrete_sequence, color_discrete_map, color_continuous_scale, range_color, color_continuous_midpoint, projection, scope, center, fitbounds, basemap_visible, title, template, width, height)\u001B[0m\n",
       "\u001B[1;32m   1051\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mchoropleth\u001B[39m(\n",
       "\u001B[1;32m   1052\u001B[0m     data_frame\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n",
       "\u001B[1;32m   1053\u001B[0m     lat\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   1085\u001B[0m     height\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n",
       "\u001B[1;32m   1086\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m go\u001B[38;5;241m.\u001B[39mFigure:\n",
       "\u001B[1;32m   1087\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   1088\u001B[0m \u001B[38;5;124;03m    In a choropleth map, each row of `data_frame` is represented by a\u001B[39;00m\n",
       "\u001B[1;32m   1089\u001B[0m \u001B[38;5;124;03m    colored region mark on a map.\u001B[39;00m\n",
       "\u001B[1;32m   1090\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 1091\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmake_figure\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1092\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mlocals\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m   1093\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconstructor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgo\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mChoropleth\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m   1094\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrace_patch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mdict\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mlocationmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocationmode\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m   1095\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/plotly/express/_core.py:1990\u001B[0m, in \u001B[0;36mmake_figure\u001B[0;34m(args, constructor, trace_patch, layout_patch)\u001B[0m\n",
       "\u001B[1;32m   1987\u001B[0m layout_patch \u001B[38;5;241m=\u001B[39m layout_patch \u001B[38;5;129;01mor\u001B[39;00m {}\n",
       "\u001B[1;32m   1988\u001B[0m apply_default_cascade(args)\n",
       "\u001B[0;32m-> 1990\u001B[0m args \u001B[38;5;241m=\u001B[39m \u001B[43mbuild_dataframe\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconstructor\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1991\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m constructor \u001B[38;5;129;01min\u001B[39;00m [go\u001B[38;5;241m.\u001B[39mTreemap, go\u001B[38;5;241m.\u001B[39mSunburst, go\u001B[38;5;241m.\u001B[39mIcicle] \u001B[38;5;129;01mand\u001B[39;00m args[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpath\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m   1992\u001B[0m     args \u001B[38;5;241m=\u001B[39m process_dataframe_hierarchy(args)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/plotly/express/_core.py:1405\u001B[0m, in \u001B[0;36mbuild_dataframe\u001B[0;34m(args, constructor)\u001B[0m\n",
       "\u001B[1;32m   1402\u001B[0m     args[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolor\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   1403\u001B[0m \u001B[38;5;66;03m# now that things have been prepped, we do the systematic rewriting of `args`\u001B[39;00m\n",
       "\u001B[0;32m-> 1405\u001B[0m df_output, wide_id_vars \u001B[38;5;241m=\u001B[39m \u001B[43mprocess_args_into_dataframe\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1406\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwide_mode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvar_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue_name\u001B[49m\n",
       "\u001B[1;32m   1407\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1409\u001B[0m \u001B[38;5;66;03m# now that `df_output` exists and `args` contains only references, we complete\u001B[39;00m\n",
       "\u001B[1;32m   1410\u001B[0m \u001B[38;5;66;03m# the special-case and wide-mode handling by further rewriting args and/or mutating\u001B[39;00m\n",
       "\u001B[1;32m   1411\u001B[0m \u001B[38;5;66;03m# df_output\u001B[39;00m\n",
       "\u001B[1;32m   1413\u001B[0m count_name \u001B[38;5;241m=\u001B[39m _escape_col_name(df_output, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcount\u001B[39m\u001B[38;5;124m\"\u001B[39m, [var_name, value_name])\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/plotly/express/_core.py:1207\u001B[0m, in \u001B[0;36mprocess_args_into_dataframe\u001B[0;34m(args, wide_mode, var_name, value_name)\u001B[0m\n",
       "\u001B[1;32m   1205\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m argument \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindex\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[1;32m   1206\u001B[0m             err_msg \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m To use the index, pass it in directly as `df.index`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m-> 1207\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(err_msg)\n",
       "\u001B[1;32m   1208\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m length \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(df_input[argument]) \u001B[38;5;241m!=\u001B[39m length:\n",
       "\u001B[1;32m   1209\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n",
       "\u001B[1;32m   1210\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAll arguments should have the same length. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   1211\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe length of column argument `df[\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m]` is \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m, whereas the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   1218\u001B[0m         )\n",
       "\u001B[1;32m   1219\u001B[0m     )\n",
       "\n",
       "\u001B[0;31mValueError\u001B[0m: Value of 'color' is not the name of a column in 'data_frame'. Expected one of ['State', \"Population at 31 March 2023\\xa0('000)\", \"Change over previous year ('000)\", 'Change over previous year (%)'] but received: Population at 31 March 2023 ('000)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\nFile \u001B[0;32m<command-4483247640063516>, line 7\u001B[0m\n\u001B[1;32m      4\u001B[0m oz \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(f)\n\u001B[1;32m      6\u001B[0m df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/dbfs/tmp/demo-streamlit/data/Australian-Bureau-of-Statistics.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 7\u001B[0m fig \u001B[38;5;241m=\u001B[39m px\u001B[38;5;241m.\u001B[39mchoropleth(df, geojson\u001B[38;5;241m=\u001B[39moz, \n\u001B[1;32m      8\u001B[0m                     color\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPopulation at 31 March 2023 (\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m000)\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      9\u001B[0m                     locations\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mState\u001B[39m\u001B[38;5;124m\"\u001B[39m, \n\u001B[1;32m     10\u001B[0m                     featureidkey\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproperties.name\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     11\u001B[0m                     color_continuous_scale\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReds\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     12\u001B[0m                     range_color\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m10000\u001B[39m),\n\u001B[1;32m     13\u001B[0m                     fitbounds \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgeojson\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     14\u001B[0m                     template \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mplotly_white\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     15\u001B[0m                    )\n\u001B[1;32m     16\u001B[0m st\u001B[38;5;241m.\u001B[39mplotly_chart(fig)\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/plotly/express/_chart_types.py:1091\u001B[0m, in \u001B[0;36mchoropleth\u001B[0;34m(data_frame, lat, lon, locations, locationmode, geojson, featureidkey, color, facet_row, facet_col, facet_col_wrap, facet_row_spacing, facet_col_spacing, hover_name, hover_data, custom_data, animation_frame, animation_group, category_orders, labels, color_discrete_sequence, color_discrete_map, color_continuous_scale, range_color, color_continuous_midpoint, projection, scope, center, fitbounds, basemap_visible, title, template, width, height)\u001B[0m\n\u001B[1;32m   1051\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mchoropleth\u001B[39m(\n\u001B[1;32m   1052\u001B[0m     data_frame\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1053\u001B[0m     lat\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1085\u001B[0m     height\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1086\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m go\u001B[38;5;241m.\u001B[39mFigure:\n\u001B[1;32m   1087\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1088\u001B[0m \u001B[38;5;124;03m    In a choropleth map, each row of `data_frame` is represented by a\u001B[39;00m\n\u001B[1;32m   1089\u001B[0m \u001B[38;5;124;03m    colored region mark on a map.\u001B[39;00m\n\u001B[1;32m   1090\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1091\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmake_figure\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1092\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mlocals\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1093\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconstructor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgo\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mChoropleth\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1094\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrace_patch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mdict\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mlocationmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocationmode\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1095\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/plotly/express/_core.py:1990\u001B[0m, in \u001B[0;36mmake_figure\u001B[0;34m(args, constructor, trace_patch, layout_patch)\u001B[0m\n\u001B[1;32m   1987\u001B[0m layout_patch \u001B[38;5;241m=\u001B[39m layout_patch \u001B[38;5;129;01mor\u001B[39;00m {}\n\u001B[1;32m   1988\u001B[0m apply_default_cascade(args)\n\u001B[0;32m-> 1990\u001B[0m args \u001B[38;5;241m=\u001B[39m \u001B[43mbuild_dataframe\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconstructor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1991\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m constructor \u001B[38;5;129;01min\u001B[39;00m [go\u001B[38;5;241m.\u001B[39mTreemap, go\u001B[38;5;241m.\u001B[39mSunburst, go\u001B[38;5;241m.\u001B[39mIcicle] \u001B[38;5;129;01mand\u001B[39;00m args[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpath\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1992\u001B[0m     args \u001B[38;5;241m=\u001B[39m process_dataframe_hierarchy(args)\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/plotly/express/_core.py:1405\u001B[0m, in \u001B[0;36mbuild_dataframe\u001B[0;34m(args, constructor)\u001B[0m\n\u001B[1;32m   1402\u001B[0m     args[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolor\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1403\u001B[0m \u001B[38;5;66;03m# now that things have been prepped, we do the systematic rewriting of `args`\u001B[39;00m\n\u001B[0;32m-> 1405\u001B[0m df_output, wide_id_vars \u001B[38;5;241m=\u001B[39m \u001B[43mprocess_args_into_dataframe\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1406\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwide_mode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvar_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue_name\u001B[49m\n\u001B[1;32m   1407\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1409\u001B[0m \u001B[38;5;66;03m# now that `df_output` exists and `args` contains only references, we complete\u001B[39;00m\n\u001B[1;32m   1410\u001B[0m \u001B[38;5;66;03m# the special-case and wide-mode handling by further rewriting args and/or mutating\u001B[39;00m\n\u001B[1;32m   1411\u001B[0m \u001B[38;5;66;03m# df_output\u001B[39;00m\n\u001B[1;32m   1413\u001B[0m count_name \u001B[38;5;241m=\u001B[39m _escape_col_name(df_output, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcount\u001B[39m\u001B[38;5;124m\"\u001B[39m, [var_name, value_name])\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/plotly/express/_core.py:1207\u001B[0m, in \u001B[0;36mprocess_args_into_dataframe\u001B[0;34m(args, wide_mode, var_name, value_name)\u001B[0m\n\u001B[1;32m   1205\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m argument \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindex\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1206\u001B[0m             err_msg \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m To use the index, pass it in directly as `df.index`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1207\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(err_msg)\n\u001B[1;32m   1208\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m length \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(df_input[argument]) \u001B[38;5;241m!=\u001B[39m length:\n\u001B[1;32m   1209\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1210\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAll arguments should have the same length. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1211\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe length of column argument `df[\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m]` is \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m, whereas the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1218\u001B[0m         )\n\u001B[1;32m   1219\u001B[0m     )\n\n\u001B[0;31mValueError\u001B[0m: Value of 'color' is not the name of a column in 'data_frame'. Expected one of ['State', \"Population at 31 March 2023\\xa0('000)\", \"Change over previous year ('000)\", 'Change over previous year (%)'] but received: Population at 31 March 2023 ('000)",
       "errorSummary": "<span class='ansi-red-fg'>ValueError</span>: Value of 'color' is not the name of a column in 'data_frame'. Expected one of ['State', \"Population at 31 March 2023\\xa0('000)\", \"Change over previous year ('000)\", 'Change over previous year (%)'] but received: Population at 31 March 2023 ('000)",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "st.title(\"Population of Australian States\")\n",
    "st.info(\"Hover over the map to see the names of the states and their population\")\n",
    "f = open('/dbfs/tmp/demo-streamlit/geo/australia.geojson')\n",
    "oz = json.load(f)\n",
    "\n",
    "df = pd.read_csv('/dbfs/tmp/demo-streamlit/data/Australian-Bureau-of-Statistics.csv')\n",
    "fig = px.choropleth(df, geojson=oz, \n",
    "                    color=\"Population at 31 March 2023 ('000)\",\n",
    "                    locations=\"State\", \n",
    "                    featureidkey=\"properties.name\",\n",
    "                    color_continuous_scale=\"Reds\",\n",
    "                    range_color=(0, 10000),\n",
    "                    fitbounds = 'geojson',\n",
    "                    template = 'plotly_white'\n",
    "                   )\n",
    "st.plotly_chart(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edddf8b2-54e3-459b-8de0-c3f0aca089fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4483247640063517>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata/europop.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[1;32m      3\u001B[0m fig \u001B[38;5;241m=\u001B[39m px\u001B[38;5;241m.\u001B[39mscatter_geo(df, scope \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124meurope\u001B[39m\u001B[38;5;124m'\u001B[39m, \n",
       "\u001B[1;32m      4\u001B[0m                     color\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPopulation (historical estimates)\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m      5\u001B[0m                     size\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPopulation (historical estimates)\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     12\u001B[0m                     title \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEuropean populations\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     13\u001B[0m                    )\n",
       "\u001B[1;32m     15\u001B[0m st\u001B[38;5;241m.\u001B[39mplotly_chart(fig)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/pandas/namespace.py:355\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(path, sep, header, names, index_col, usecols, squeeze, mangle_dupe_cols, dtype, nrows, parse_dates, quotechar, escapechar, comment, encoding, **options)\u001B[0m\n",
       "\u001B[1;32m    353\u001B[0m     column_labels \u001B[38;5;241m=\u001B[39m {col: col \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m sdf\u001B[38;5;241m.\u001B[39mcolumns}\n",
       "\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 355\u001B[0m     sdf \u001B[38;5;241m=\u001B[39m \u001B[43mreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    356\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_list_like(names):\n",
       "\u001B[1;32m    357\u001B[0m         names \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(names)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:43\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     39\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n",
       "\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
       "\u001B[1;32m     41\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(_local, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogging\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m _local\u001B[38;5;241m.\u001B[39mlogging:\n",
       "\u001B[1;32m     42\u001B[0m         \u001B[38;5;66;03m# no need to log since this should be internal call.\u001B[39;00m\n",
       "\u001B[0;32m---> 43\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     44\u001B[0m     _local\u001B[38;5;241m.\u001B[39mlogging \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m     45\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:740\u001B[0m, in \u001B[0;36mDataFrameReader.csv\u001B[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001B[0m\n",
       "\u001B[1;32m    738\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(path) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n",
       "\u001B[1;32m    739\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[0;32m--> 740\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_spark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPythonUtils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtoSeq\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n",
       "\u001B[1;32m    741\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, RDD):\n",
       "\u001B[1;32m    743\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfunc\u001B[39m(iterator):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:194\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    190\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    192\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    193\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 194\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mIllegalArgumentException\u001B[0m: Path must be absolute: data/europop.csv"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\nFile \u001B[0;32m<command-4483247640063517>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata/europop.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      3\u001B[0m fig \u001B[38;5;241m=\u001B[39m px\u001B[38;5;241m.\u001B[39mscatter_geo(df, scope \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124meurope\u001B[39m\u001B[38;5;124m'\u001B[39m, \n\u001B[1;32m      4\u001B[0m                     color\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPopulation (historical estimates)\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      5\u001B[0m                     size\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPopulation (historical estimates)\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     12\u001B[0m                     title \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEuropean populations\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     13\u001B[0m                    )\n\u001B[1;32m     15\u001B[0m st\u001B[38;5;241m.\u001B[39mplotly_chart(fig)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/pandas/namespace.py:355\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(path, sep, header, names, index_col, usecols, squeeze, mangle_dupe_cols, dtype, nrows, parse_dates, quotechar, escapechar, comment, encoding, **options)\u001B[0m\n\u001B[1;32m    353\u001B[0m     column_labels \u001B[38;5;241m=\u001B[39m {col: col \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m sdf\u001B[38;5;241m.\u001B[39mcolumns}\n\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 355\u001B[0m     sdf \u001B[38;5;241m=\u001B[39m \u001B[43mreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    356\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_list_like(names):\n\u001B[1;32m    357\u001B[0m         names \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(names)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:43\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m     41\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(_local, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogging\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m _local\u001B[38;5;241m.\u001B[39mlogging:\n\u001B[1;32m     42\u001B[0m         \u001B[38;5;66;03m# no need to log since this should be internal call.\u001B[39;00m\n\u001B[0;32m---> 43\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     44\u001B[0m     _local\u001B[38;5;241m.\u001B[39mlogging \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     45\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:740\u001B[0m, in \u001B[0;36mDataFrameReader.csv\u001B[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001B[0m\n\u001B[1;32m    738\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(path) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n\u001B[1;32m    739\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 740\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_spark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPythonUtils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtoSeq\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    741\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, RDD):\n\u001B[1;32m    743\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfunc\u001B[39m(iterator):\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:194\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    190\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    192\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    193\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 194\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mIllegalArgumentException\u001B[0m: Path must be absolute: data/europop.csv",
       "errorSummary": "<span class='ansi-red-fg'>IllegalArgumentException</span>: Path must be absolute: data/europop.csv",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('data/europop.csv')\n",
    "\n",
    "fig = px.scatter_geo(df, scope = 'europe', \n",
    "                    color=\"Population (historical estimates)\",\n",
    "                    size=\"Population (historical estimates)\",\n",
    "                    locations=\"Code\", \n",
    "                    hover_name = 'Entity',\n",
    "                    color_continuous_scale=\"Purples\",\n",
    "                    range_color=(0, 100000000),\n",
    "                    fitbounds = 'locations',\n",
    "                    template = 'plotly_white',\n",
    "                    title = \"European populations\"\n",
    "                   )\n",
    "\n",
    "st.plotly_chart(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dc61f42-c3a8-4952-887c-b58923ebbb13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4483247640063522>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m fig \u001B[38;5;241m=\u001B[39m px\u001B[38;5;241m.\u001B[39mchoropleth(df_total[df_total[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mYear\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m==\u001B[39myear], \n",
       "\u001B[1;32m      2\u001B[0m                     locations\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCode\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m      3\u001B[0m                     color\u001B[38;5;241m=\u001B[39mcol,\n",
       "\u001B[1;32m      4\u001B[0m                     hover_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEntity\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m      5\u001B[0m                     range_color\u001B[38;5;241m=\u001B[39m(\u001B[38;5;28mmin\u001B[39m,\u001B[38;5;28mmax\u001B[39m),\n",
       "\u001B[1;32m      6\u001B[0m                     scope\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mworld\u001B[39m\u001B[38;5;124m'\u001B[39m,\n",
       "\u001B[1;32m      7\u001B[0m                     projection\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morthographic\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m      8\u001B[0m                     color_continuous_scale\u001B[38;5;241m=\u001B[39mpx\u001B[38;5;241m.\u001B[39mcolors\u001B[38;5;241m.\u001B[39msequential\u001B[38;5;241m.\u001B[39mReds)\n",
       "\u001B[1;32m      9\u001B[0m st\u001B[38;5;241m.\u001B[39mplotly_chart(fig)\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'df_total' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-4483247640063522>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m fig \u001B[38;5;241m=\u001B[39m px\u001B[38;5;241m.\u001B[39mchoropleth(df_total[df_total[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mYear\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m==\u001B[39myear], \n\u001B[1;32m      2\u001B[0m                     locations\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCode\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      3\u001B[0m                     color\u001B[38;5;241m=\u001B[39mcol,\n\u001B[1;32m      4\u001B[0m                     hover_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEntity\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      5\u001B[0m                     range_color\u001B[38;5;241m=\u001B[39m(\u001B[38;5;28mmin\u001B[39m,\u001B[38;5;28mmax\u001B[39m),\n\u001B[1;32m      6\u001B[0m                     scope\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mworld\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m      7\u001B[0m                     projection\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morthographic\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      8\u001B[0m                     color_continuous_scale\u001B[38;5;241m=\u001B[39mpx\u001B[38;5;241m.\u001B[39mcolors\u001B[38;5;241m.\u001B[39msequential\u001B[38;5;241m.\u001B[39mReds)\n\u001B[1;32m      9\u001B[0m st\u001B[38;5;241m.\u001B[39mplotly_chart(fig)\n\n\u001B[0;31mNameError\u001B[0m: name 'df_total' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'df_total' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = px.choropleth(df_total[df_total['Year']==year], \n",
    "                    locations=\"Code\",\n",
    "                    color=col,\n",
    "                    hover_name=\"Entity\",\n",
    "                    range_color=(min,max),\n",
    "                    scope= 'world',\n",
    "                    projection=\"orthographic\",\n",
    "                    color_continuous_scale=px.colors.sequential.Reds)\n",
    "st.plotly_chart(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc3e9e5f-2e36-4215-95c1-1aaa865c023e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4483247640063523>, line 13\u001B[0m\n",
       "\u001B[1;32m      7\u001B[0m st\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\"\"\u001B[39m\u001B[38;5;124mUse the slider to select a year to display\u001B[39m\n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;124m           the total emissions for each country. \u001B[39m\n",
       "\u001B[1;32m      9\u001B[0m \u001B[38;5;124m           Scroll down to see an interactive 3D representation.\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m)\n",
       "\u001B[1;32m     11\u001B[0m col1, col2 \u001B[38;5;241m=\u001B[39m st\u001B[38;5;241m.\u001B[39mcolumns(\u001B[38;5;241m2\u001B[39m)\n",
       "\u001B[0;32m---> 13\u001B[0m df_total \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata/co2_total.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[1;32m     14\u001B[0m col \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAnnual CO₂ emissions\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
       "\u001B[1;32m     15\u001B[0m \u001B[38;5;28mmax\u001B[39m \u001B[38;5;241m=\u001B[39m df_total[col]\u001B[38;5;241m.\u001B[39mmax()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/pandas/namespace.py:355\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(path, sep, header, names, index_col, usecols, squeeze, mangle_dupe_cols, dtype, nrows, parse_dates, quotechar, escapechar, comment, encoding, **options)\u001B[0m\n",
       "\u001B[1;32m    353\u001B[0m     column_labels \u001B[38;5;241m=\u001B[39m {col: col \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m sdf\u001B[38;5;241m.\u001B[39mcolumns}\n",
       "\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 355\u001B[0m     sdf \u001B[38;5;241m=\u001B[39m \u001B[43mreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    356\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_list_like(names):\n",
       "\u001B[1;32m    357\u001B[0m         names \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(names)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:43\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     39\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n",
       "\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
       "\u001B[1;32m     41\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(_local, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogging\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m _local\u001B[38;5;241m.\u001B[39mlogging:\n",
       "\u001B[1;32m     42\u001B[0m         \u001B[38;5;66;03m# no need to log since this should be internal call.\u001B[39;00m\n",
       "\u001B[0;32m---> 43\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     44\u001B[0m     _local\u001B[38;5;241m.\u001B[39mlogging \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m     45\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:740\u001B[0m, in \u001B[0;36mDataFrameReader.csv\u001B[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001B[0m\n",
       "\u001B[1;32m    738\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(path) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n",
       "\u001B[1;32m    739\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[0;32m--> 740\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_spark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPythonUtils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtoSeq\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n",
       "\u001B[1;32m    741\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, RDD):\n",
       "\u001B[1;32m    743\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfunc\u001B[39m(iterator):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:194\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    190\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    192\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    193\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 194\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mIllegalArgumentException\u001B[0m: Path must be absolute: data/co2_total.csv"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\nFile \u001B[0;32m<command-4483247640063523>, line 13\u001B[0m\n\u001B[1;32m      7\u001B[0m st\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\"\"\u001B[39m\u001B[38;5;124mUse the slider to select a year to display\u001B[39m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;124m           the total emissions for each country. \u001B[39m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;124m           Scroll down to see an interactive 3D representation.\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m)\n\u001B[1;32m     11\u001B[0m col1, col2 \u001B[38;5;241m=\u001B[39m st\u001B[38;5;241m.\u001B[39mcolumns(\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m---> 13\u001B[0m df_total \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata/co2_total.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     14\u001B[0m col \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAnnual CO₂ emissions\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28mmax\u001B[39m \u001B[38;5;241m=\u001B[39m df_total[col]\u001B[38;5;241m.\u001B[39mmax()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/pandas/namespace.py:355\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(path, sep, header, names, index_col, usecols, squeeze, mangle_dupe_cols, dtype, nrows, parse_dates, quotechar, escapechar, comment, encoding, **options)\u001B[0m\n\u001B[1;32m    353\u001B[0m     column_labels \u001B[38;5;241m=\u001B[39m {col: col \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m sdf\u001B[38;5;241m.\u001B[39mcolumns}\n\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 355\u001B[0m     sdf \u001B[38;5;241m=\u001B[39m \u001B[43mreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    356\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_list_like(names):\n\u001B[1;32m    357\u001B[0m         names \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(names)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:43\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m     41\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(_local, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogging\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m _local\u001B[38;5;241m.\u001B[39mlogging:\n\u001B[1;32m     42\u001B[0m         \u001B[38;5;66;03m# no need to log since this should be internal call.\u001B[39;00m\n\u001B[0;32m---> 43\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     44\u001B[0m     _local\u001B[38;5;241m.\u001B[39mlogging \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     45\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:740\u001B[0m, in \u001B[0;36mDataFrameReader.csv\u001B[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001B[0m\n\u001B[1;32m    738\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(path) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n\u001B[1;32m    739\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 740\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_spark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPythonUtils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtoSeq\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    741\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, RDD):\n\u001B[1;32m    743\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfunc\u001B[39m(iterator):\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:194\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    190\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    192\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    193\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 194\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mIllegalArgumentException\u001B[0m: Path must be absolute: data/co2_total.csv",
       "errorSummary": "<span class='ansi-red-fg'>IllegalArgumentException</span>: Path must be absolute: data/co2_total.csv",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "st.set_page_config(layout=\"wide\")\n",
    "\n",
    "st.title(\"CO2 Emissions\")\n",
    "st.write(\"\"\"The following maps display the CO2 emissions for a\n",
    "            range of countries over a range of time\"\"\")\n",
    "\n",
    "st.info(\"\"\"Use the slider to select a year to display\n",
    "           the total emissions for each country. \n",
    "           Scroll down to see an interactive 3D representation.\"\"\")\n",
    "\n",
    "col1, col2 = st.columns(2)\n",
    "\n",
    "df_total = pd.read_csv('data/co2_total.csv')\n",
    "col = 'Annual CO₂ emissions'\n",
    "max = df_total[col].max()\n",
    "min = df_total[col].min()\n",
    "\n",
    "# To get the whole range replace 1950 with the comment that follows it\n",
    "first_year = 1950 #df_total['Year'].min()\n",
    "last_year = df_total['Year'].max()\n",
    "year = st.slider('Select year',first_year,last_year, key=col)\n",
    "\n",
    "col1.write(\"\"\"This map uses the 'Natural Earth' projection\"\"\")\n",
    "fig = px.choropleth(df_total[df_total['Year']==year], \n",
    "                    locations=\"Code\",\n",
    "                    color=col,\n",
    "                    hover_name=\"Entity\",\n",
    "                    range_color=(min,max),\n",
    "                    scope= 'world',\n",
    "                    projection=\"natural earth\",\n",
    "                    color_continuous_scale=px.colors.sequential.Reds)\n",
    "col1.plotly_chart(fig)\n",
    "\n",
    "col2.write(\"\"\"This map uses the 'Orthographic' projection.\n",
    "Click on the globe and move the pointer to rotate it.\n",
    "\"\"\")\n",
    "\n",
    "fig = px.choropleth(df_total[df_total['Year']==year], locations=\"Code\",\n",
    "                    color=col,\n",
    "                    hover_name=\"Entity\",\n",
    "                    range_color=(min,max),\n",
    "                    scope= 'world',\n",
    "                    projection=\"orthographic\",\n",
    "                    color_continuous_scale=px.colors.sequential.Reds)\n",
    "col2.plotly_chart(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e23658b7-e78c-4cb1-88ce-7da0359dd46c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;36m  File \u001B[0;32m<command-4483247640063524>, line 2\u001B[0;36m\u001B[0m\n",
       "\u001B[0;31m    c = st.multiselect('Add a country:', countries, default=['United States', 'China', 'Russia', 'Germany'])\u001B[0m\n",
       "\u001B[0m    ^\u001B[0m\n",
       "\u001B[0;31mIndentationError\u001B[0m\u001B[0;31m:\u001B[0m unexpected indent\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;36m  File \u001B[0;32m<command-4483247640063524>, line 2\u001B[0;36m\u001B[0m\n\u001B[0;31m    c = st.multiselect('Add a country:', countries, default=['United States', 'China', 'Russia', 'Germany'])\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mIndentationError\u001B[0m\u001B[0;31m:\u001B[0m unexpected indent\n",
       "errorSummary": "<span class='ansi-red-fg'>IndentationError</span>: unexpected indent (command-4483247640063524-675239851, line 2)",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# add/subtract from the selected countries\n",
    "    c = st.multiselect('Add a country:', countries, default=['United States', 'China', 'Russia', 'Germany'])\n",
    "    tab1, tab2 = col2.tabs([\"Graph\", \"Table\"])\n",
    "\n",
    "    with tab1:\n",
    "        # plot a line graph of emissions for selected countries\n",
    "        fig = px.line(df_total[df_total['Entity'].isin(c)], x='Year', y='Annual CO₂ emissions', color = 'Entity')\n",
    "        st.plotly_chart(fig, use_container_width=True)\n",
    "    with tab2:\n",
    "        table = df_total[df_total['Year']==st.session_state['year']]\n",
    "        st.dataframe(table[table['Entity'].isin(c)], use_container_width=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3362107d-d6fa-437d-aab8-12f3df196b65",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4483247640063528,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Demos - Geo - Plotly and streamlit",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
